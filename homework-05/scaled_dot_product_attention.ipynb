{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Scaled Dot-Product Attention\n",
    "\n",
    "## 1. `scaled_dot_product_attention.py`\n",
    "\n",
    "This module implements the **Scaled Dot-Product Attention** mechanism using **NumPy**, following the formulation used in Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ Formula\n",
    "\n",
    "For a single attention head:\n",
    "\n",
    "[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "| Symbol | Meaning      | Shape                   |\n",
    "| ------ | ------------ | ----------------------- |\n",
    "| **Q**  | Query matrix | `(..., seq_len_q, d_k)` |\n",
    "| **K**  | Key matrix   | `(..., seq_len_k, d_k)` |\n",
    "| **V**  | Value matrix | `(..., seq_len_k, d_v)` |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Features\n",
    "\n",
    "This implementation:\n",
    "\n",
    "* Uses **NumPy** for all matrix operations\n",
    "* Computes a **numerically stable softmax**\n",
    "* Produces:\n",
    "\n",
    "  * **Attention weights** â†’ shape `(..., seq_len_q, seq_len_k)`\n",
    "  * **Context vectors** â†’ shape `(..., seq_len_q, d_v)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a numerically stable softmax along a given axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array of scores (logits).\n",
    "    axis : int, optional\n",
    "        Axis along which to apply softmax. Defaults to -1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Softmax-normalized probabilities with the same shape as `x`.\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability: softmax(x) == softmax(x - max(x))\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    shifted = x - x_max\n",
    "\n",
    "    exp_x = np.exp(shifted)\n",
    "    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    q: np.ndarray,\n",
    "    k: np.ndarray,\n",
    "    v: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : np.ndarray\n",
    "        Query matrix. Shape: (..., seq_len_q, d_k)\n",
    "    k : np.ndarray\n",
    "        Key matrix. Shape: (..., seq_len_k, d_k)\n",
    "    v : np.ndarray\n",
    "        Value matrix. Shape: (..., seq_len_k, d_v)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        A tuple containing:\n",
    "        - attention_weights: shape (..., seq_len_q, seq_len_k)\n",
    "        - context: shape (..., seq_len_q, d_v)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The leading dimensions `...` (for example, batch size or number of heads)\n",
    "    are kept and broadcast correctly as long as `q`, `k`, and `v` share the\n",
    "    same leading dimensions.\n",
    "    \"\"\"\n",
    "    # Get the dimensionality of the keys (d_k) from the last dimension of K\n",
    "    d_k = k.shape[-1]\n",
    "\n",
    "    # 1. Compute raw attention scores: Q K^T\n",
    "    #    Using matrix multiplication on the last two dimensions:\n",
    "    #    (..., seq_len_q, d_k) @ (..., d_k, seq_len_k) -> (..., seq_len_q, seq_len_k)\n",
    "    scores = np.matmul(q, np.swapaxes(k, -1, -2))\n",
    "\n",
    "    # 2. Scale scores by sqrt(d_k) to keep gradients stable\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "\n",
    "    # 3. Normalize scores into probabilities using softmax\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "\n",
    "    # 4. Compute the weighted sum of values:\n",
    "    #    (..., seq_len_q, seq_len_k) @ (..., seq_len_k, d_v)\n",
    "    #    -> (..., seq_len_q, d_v)\n",
    "    context = np.matmul(attention_weights, v)\n",
    "\n",
    "    return attention_weights, context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: (2, 4, 4)\n",
      "Context shape: (2, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple demo to show shapes and that the function runs correctly.\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "# Random toy inputs\n",
    "q = np.random.randn(batch_size, seq_len, d_k)\n",
    "k = np.random.randn(batch_size, seq_len, d_k)\n",
    "v = np.random.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "attn_weights, context = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # (2, 4, 4)\n",
    "print(\"Context shape:\", context.shape)                # (2, 4, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
