{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Scaled Dot-Product Attention\n",
    "\n",
    "## 1. `scaled_dot_product_attention.ipynb`\n",
    "\n",
    "This module implements the **Scaled Dot-Product Attention** mechanism using **NumPy**, following the formulation used in Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ Formula\n",
    "\n",
    "For a single attention head:\n",
    "\n",
    "[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "| Symbol | Meaning      | Shape                   |\n",
    "| ------ | ------------ | ----------------------- |\n",
    "| **Q**  | Query matrix | `(..., seq_len_q, d_k)` |\n",
    "| **K**  | Key matrix   | `(..., seq_len_k, d_k)` |\n",
    "| **V**  | Value matrix | `(..., seq_len_k, d_v)` |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Features\n",
    "\n",
    "This implementation:\n",
    "\n",
    "* Uses **NumPy** for all matrix operations\n",
    "* Computes a **numerically stable softmax**\n",
    "* Produces: **Weights** and **Vector**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Q matrix (example: [[1,0,1]] ):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter K matrix (example: [[1,0,1],[0,1,0]] ):\n",
      "Enter V matrix (example: [[5,5],[1,1]] ):\n",
      "\n",
      "Attention Weights:\n",
      " [[0.76036844 0.23963156]]\n",
      "\n",
      "Context Vector:\n",
      " [[4.04147377 4.04147377]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)     # QK^T / sqrt(d_k)\n",
    "    attention_weights = softmax(scores)           # softmax\n",
    "    context_vector = np.matmul(attention_weights, V)\n",
    "    return attention_weights, context_vector\n",
    "\n",
    "\n",
    "# ---- USER INPUT SECTION ----\n",
    "\n",
    "print(\"Enter Q matrix (example: [[1,0,1]] ):\")\n",
    "Q = np.array(eval(input(\"Q = \")))\n",
    "\n",
    "print(\"Enter K matrix (example: [[1,0,1],[0,1,0]] ):\")\n",
    "K = np.array(eval(input(\"K = \")))\n",
    "\n",
    "print(\"Enter V matrix (example: [[5,5],[1,1]] ):\")\n",
    "V = np.array(eval(input(\"V = \")))\n",
    "\n",
    "attn, ctx = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"\\nAttention Weights:\\n\", attn)\n",
    "print(\"\\nContext Vector:\\n\", ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
