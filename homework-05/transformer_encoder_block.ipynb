{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `transformer_encoder_block.ipynb`\n",
    "\n",
    "Implements a simplified **Transformer Encoder Block** using **PyTorch**.\n",
    "\n",
    "### ðŸ”§ Components\n",
    "\n",
    "* **Multi-head self-attention** (`nn.MultiheadAttention`)\n",
    "* **Feed-forward network**\n",
    "  `Linear â†’ ReLU â†’ Linear`\n",
    "* **Residual (skip) connections**\n",
    "* **Layer normalization** (Add & Norm)\n",
    "\n",
    "### âš™ï¸ Configuration\n",
    "\n",
    "* `d_model = 128`\n",
    "* `num_heads = 8`\n",
    "\n",
    "### ðŸ§ª Included Test\n",
    "\n",
    "* **Batch size:** 32 sentences\n",
    "* **Sequence length:** 10 tokens\n",
    "* **Embedding dimension:** 128\n",
    "\n",
    "It prints the output shape to verify correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Transformer encoder block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_model : int\n",
    "        Dimensionality of input embeddings (and the attention model).\n",
    "    num_heads : int\n",
    "        Number of attention heads in the multi-head attention layer.\n",
    "    dim_feedforward : int, optional\n",
    "        Hidden dimension of the feed-forward network. Typically 2-4x d_model.\n",
    "    dropout : float, optional\n",
    "        Dropout probability applied after attention and feed-forward layers.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x : torch.Tensor\n",
    "        Input tensor of shape (batch_size, seq_len, d_model).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Output tensor of shape (batch_size, seq_len, d_model).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 128,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head self-attention layer.\n",
    "        # Using batch_first=True means inputs are (batch, seq, d_model)\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Feed-forward network: Linear -> ReLU -> Linear\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "\n",
    "        # Layer normalization layers for \"Add & Norm\"\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout layers applied after attention and feed-forward sublayers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer encoder block.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, seq_len, d_model).\n",
    "        src_key_padding_mask : torch.Tensor, optional\n",
    "            Boolean mask of shape (batch_size, seq_len) where True indicates\n",
    "            positions that should be ignored (e.g., padding tokens).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor of shape (batch_size, seq_len, d_model) after applying\n",
    "            self-attention, feed-forward network, and Add & Norm layers.\n",
    "        \"\"\"\n",
    "        # ---- 1. Multi-head self-attention sublayer ----\n",
    "        # For self-attention: query = key = value = x\n",
    "        attn_output, _ = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            need_weights=False,  # We only need the transformed output here\n",
    "        )\n",
    "\n",
    "        # Apply dropout to attention output\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "\n",
    "        # Residual connection: add the original input (skip connection)\n",
    "        x = x + attn_output\n",
    "\n",
    "        # Layer normalization (first Add & Norm)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # ---- 2. Position-wise feed-forward sublayer ----\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Dropout after feed-forward\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        # Second residual connection\n",
    "        x = x + ffn_output\n",
    "\n",
    "        # Second layer normalization (second Add & Norm)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([32, 10, 128])\n",
      "Output shape: torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Quick shape test to satisfy the assignment's sub-task (c).\n",
    "\n",
    "# Parameters from the problem statement\n",
    "batch_size = 32     # number of sentences\n",
    "seq_len = 10        # number of tokens per sentence\n",
    "d_model = 128       # embedding dimension (must match the block)\n",
    "\n",
    "# Create random input: (batch_size, seq_len, d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Initialize our encoder block\n",
    "encoder_block = TransformerEncoderBlock(d_model=d_model, num_heads=8)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder_block(x)\n",
    "\n",
    "print(\"Input shape :\", x.shape)       # torch.Size([32, 10, 128])\n",
    "print(\"Output shape:\", output.shape)  # torch.Size([32, 10, 128])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
